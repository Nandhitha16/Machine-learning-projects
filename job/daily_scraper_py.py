# -*- coding: utf-8 -*-
"""daily_scraper.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZaZKEu_CCwDKeT9tYA8sYtfDqSsE9aEo
"""
import os
import subprocess
import sys

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

try:
    from bs4 import BeautifulSoup
except ImportError:
    install("beautifulsoup4")
    from bs4 import BeautifulSoup

try:
    import requests
except ImportError:
    install("requests")
    import requests

import pandas as pd
import pickle
from your_module import scrape_karkidi_jobs, load_model_from_pkl  # Adjust import if needed

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"Scraping page: {page}")
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")

        job_blocks = soup.find_all("div", class_="ads-details")
        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                experience = job.find("p", class_="emp-exp").get_text(strip=True)
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"Error parsing job block: {e}")
                continue

        time.sleep(1)  # Be nice to the server

    return pd.DataFrame(jobs_list)

# Example use:
if __name__ == "__main__":
    df_jobs = scrape_karkidi_jobs(keyword="data science", pages=2)
    print(df_jobs.head())

# preprocess_and_cluster.py

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import joblib

# Load data
df = pd.read_csv("/content/karkidi_jobs .csv")

# Fill missing skill data
df["Skills"] = df["Skills"].fillna("")

# TF-IDF vectorization
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df["Skills"])

# KMeans Clustering
kmeans = KMeans(n_clusters=5, random_state=42)
df["Cluster"] = kmeans.fit_predict(X)

# Save the model and vectorizer
joblib.dump(kmeans, "kmeans_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")
df.to_csv("clustered_jobs.csv", index=False)

print("Clustering complete and model saved.")
pd.read_csv("/content/clustered_jobs.csv")

# automate_daily_scrape.py

from datetime import datetime
import pandas as pd

df_new = scrape_karkidi_jobs(keyword="data science", pages=2)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
df_new.to_csv(f"scraped_jobs_{timestamp}.csv", index=False)

import pandas as pd
from datetime import datetime
import joblib
#from scrape_karkidi import scrape_karkidi_jobs  # Assuming your scraper is saved as scrape_karkidi.py

# Load model & vectorizer
vectorizer = joblib.load("vectorizer.pkl")
kmeans = joblib.load("kmeans_model.pkl")

# User's preferred cluster (manually determined or saved from a config)
user_preferred_cluster = 2  # Example

# Step 1: Scrape latest jobs
df_new = scrape_karkidi_jobs(keyword="data science", pages=2)
df_new["Skills"] = df_new["Skills"].fillna("")

# Step 2: Vectorize and predict cluster
X_new = vectorizer.transform(df_new["Skills"])
df_new["Cluster"] = kmeans.predict(X_new)

# Step 3: Filter matches
matched_jobs = df_new[df_new["Cluster"] == user_preferred_cluster]

# Step 4: Save all + matched jobs
timestamp = datetime.now().strftime("%Y%m%d_%H%M")
df_new.to_csv(f"jobs_all_{timestamp}.csv", index=False)
matched_jobs.to_csv(f"matched_jobs_{timestamp}.csv", index=False)

print(f"Found {len(matched_jobs)} matching jobs in your preferred category.")
